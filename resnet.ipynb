{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('DNNProject/dataset/Data_Entry_2017.csv')\n",
    "data = data.rename(columns={'Finding Label': 'Finding Labels'})\n",
    "\n",
    "def filterData(data):\n",
    "    filtered_data = data[\n",
    "    ~data['Finding Labels'].str.contains('Hernia', na=False) &\n",
    "    ~data['Finding Labels'].str.contains('Pneumonia', na=False) &\n",
    "    ~data['Finding Labels'].str.contains('Edema', na=False) &\n",
    "    ~data['Finding Labels'].str.contains('Cardiomegaly', na=False) &\n",
    "    ~data['Finding Labels'].str.contains('Consolidation', na=False) &\n",
    "    ~data['Finding Labels'].str.contains('Pleural_Thickening', na=False) &\n",
    "    ~data['Finding Labels'].str.contains('Fibrosis', na=False) &\n",
    "    ~data['Finding Labels'].str.contains('Emphysema', na=False) &\n",
    "    #~data['Finding Labels'].str.contains('Nodule', na=False) &\n",
    "    ~data['Finding Labels'].str.contains('Pneumothorax', na=False) &\n",
    "    ~data['Finding Labels'].str.contains('Mass', na=False) &\n",
    "    ~data['Finding Labels'].str.contains('No Finding', na=False) &\n",
    "    ~data['Finding Labels'].str.contains(r'\\|', na=False)\n",
    "    ]\n",
    "    return filtered_data\n",
    "\n",
    "filtered_data = filterData(data)\n",
    "filtered_data = filtered_data[['Image Index', 'Finding Labels']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_image_path_nested(image_index, base_dir):\n",
    "    #global subfolders_with_valid_paths, subfolders_without_valid_paths\n",
    "    subfolders = sorted(os.listdir(base_dir))  # Sort subfolders for consistency\n",
    "    found_path = None  # Initialize as None\n",
    "\n",
    "    for subfolder in subfolders:\n",
    "        subfolder_path = os.path.join(base_dir, subfolder)\n",
    "        if os.path.isdir(subfolder_path):  # Check if it's a directory\n",
    "            # Look for a nested 'images' folder\n",
    "            nested_images_path = os.path.join(subfolder_path, 'images')\n",
    "            if os.path.exists(nested_images_path) and os.path.isdir(nested_images_path):\n",
    "                # Check if the image exists in the nested folder\n",
    "                image_path = os.path.join(nested_images_path, image_index)\n",
    "                if os.path.exists(image_path):\n",
    "                    #subfolders_with_valid_paths.add(subfolder)  # Mark subfolder as valid\n",
    "                    found_path = image_path\n",
    "                    break  # Stop searching once the image is found\n",
    "            #else:\n",
    "                #subfolders_without_valid_paths.add(subfolder)  # Mark as invalid\n",
    "    return found_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'DNNProject/dataset'\n",
    "\n",
    "subfolders_with_valid_paths = set()\n",
    "subfolders_without_valid_paths = set()\n",
    "\n",
    "filtered_data['Image Path'] = filtered_data['Image Index'].apply(lambda x: find_image_path_nested(x, base_dir))\n",
    "\n",
    "\n",
    "print(f\"Number of None paths: {filtered_data['Image Path'].isna().sum()}\")\n",
    "print(f\"Number of valid paths: {filtered_data['Image Path'].notna().sum()}\")\n",
    "\n",
    "\n",
    "filtered_data = filtered_data.dropna(subset=['Image Path'])\n",
    "\n",
    "\n",
    "print(filtered_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class_counts = filtered_data['Finding Labels'].value_counts()\n",
    "\n",
    "# Plot the counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "class_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Number of Samples per Class', fontsize=16)\n",
    "plt.xlabel('Class', fontsize=14)\n",
    "plt.ylabel('Number of Samples', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate labels for better readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('results/class_dist.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "train_df, valid_df = train_test_split(\n",
    "    filtered_data,\n",
    "    test_size=0.20, \n",
    "    random_state=42,\n",
    "    stratify=filtered_data['Finding Labels'] \n",
    ")\n",
    "\n",
    "print(\"Training set distribution:\")\n",
    "print(train_df['Finding Labels'].value_counts())\n",
    "print(\"\\nValidation set distribution:\")\n",
    "print(valid_df['Finding Labels'].value_counts())\n",
    "\n",
    "\n",
    "label_to_index = {label: idx for idx, label in enumerate(filtered_data['Finding Labels'].unique())}\n",
    "index_to_label = {idx: label for label, idx in label_to_index.items()}  # Reverse mapping\n",
    "\n",
    "train_df['Finding Labels'] = train_df['Finding Labels'].map(label_to_index)\n",
    "valid_df['Finding Labels'] = valid_df['Finding Labels'].map(label_to_index)\n",
    "\n",
    "print(\"\\nLabel to Index Mapping:\")\n",
    "for label, idx in label_to_index.items():\n",
    "    print(f\"Label: {label}, Index: {idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def resample_data(train_df, label_name,label_to_index, \n",
    "                  downsample=True, upsample=True, \n",
    "                  target_downsample_count=500, target_upsample_count=160):\n",
    "   \n",
    "    \n",
    "    no_finding_label = label_to_index[label_name]\n",
    "    no_finding_samples = train_df[train_df['Finding Labels'] == no_finding_label]\n",
    "    other_classes = train_df[train_df['Finding Labels'] != no_finding_label]\n",
    "\n",
    "  \n",
    "    if downsample:\n",
    "        print(f\"Downsampling {label_name} to {target_downsample_count} samples.\")\n",
    "        downsampled_no_finding = no_finding_samples.sample(\n",
    "            n=target_downsample_count, random_state=42\n",
    "        )\n",
    "    else:\n",
    "        downsampled_no_finding = no_finding_samples\n",
    "\n",
    "  \n",
    "    upsampled_dfs = []\n",
    "    if upsample:\n",
    "        class_counts = other_classes['Finding Labels'].value_counts()\n",
    "        classes_to_upsample = class_counts[class_counts < target_upsample_count].index\n",
    "        print(f\"Upsampling classes: {classes_to_upsample} to {target_upsample_count} samples each.\")\n",
    "\n",
    "        for class_label in classes_to_upsample:\n",
    "            class_rows = other_classes[other_classes['Finding Labels'] == class_label]\n",
    "            upsampled_rows = class_rows.sample(\n",
    "                n=target_upsample_count, replace=True, random_state=42\n",
    "            )\n",
    "            upsampled_dfs.append(upsampled_rows)\n",
    "\n",
    "   \n",
    "    remaining_classes = other_classes[~other_classes['Finding Labels'].isin(classes_to_upsample)] if upsample else other_classes\n",
    "    final_train_df = pd.concat([downsampled_no_finding, remaining_classes] + upsampled_dfs)\n",
    "\n",
    "   \n",
    "    final_train_df = final_train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    print(\"Resampling completed.\")\n",
    "    return final_train_df\n",
    "\n",
    "\"\"\"train_df = resample_data(\n",
    "    train_df=train_df,\n",
    "    label_name = 'Infiltration',\n",
    "    label_to_index=label_to_index,\n",
    "    downsample=True,\n",
    "    upsample=False,  # Disable upsampling\n",
    "    target_downsample_count=300\n",
    ")\"\"\"\n",
    "\n",
    "train_df = resample_data(\n",
    "    train_df=train_df,\n",
    "    label_name = 'Infiltration',\n",
    "    label_to_index=label_to_index,\n",
    "    downsample=True,\n",
    "    upsample=False,  # Disable upsampling\n",
    "    target_downsample_count=4000\n",
    ")\n",
    "\n",
    "class_counts_after_sampling = train_df['Finding Labels'].value_counts()\n",
    "\n",
    "print(\"Label | Index | Number of Samples\")\n",
    "print(\"-\" * 40)\n",
    "for label, idx in label_to_index.items():\n",
    "    count = class_counts_after_sampling[idx] if idx in class_counts_after_sampling else 0\n",
    "    print(f\"{label:<15} | {idx:<5} | {count:<10}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "    \n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_path = self.dataframe.iloc[idx]['Image Path']\n",
    "        label = self.dataframe.iloc[idx]['Finding Labels']\n",
    "\n",
    "    \n",
    "        image = Image.open(img_path).convert(\"RGB\")  \n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return image, label_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "#\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(train_transforms.transforms)\n",
    "\n",
    "\n",
    "train_dataset = ChestXrayDataset(train_df, transform=train_transforms)\n",
    "val_dataset = ChestXrayDataset(valid_df, transform=val_transforms)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4,pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class ResBlocks(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_bottleneck=False, downsample=False):\n",
    "        super(ResBlocks, self).__init__()\n",
    "        stride = 2 if downsample else 1\n",
    "\n",
    "        if use_bottleneck:\n",
    "            self.block = self.bottleneck_block(in_channels, out_channels, stride)\n",
    "        else:\n",
    "            self.block = self.residual_block(in_channels, out_channels, stride)\n",
    "\n",
    "        # Oppdater shortcut for å bruke riktig stride hvis det er downsample\n",
    "        if downsample or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "    def residual_block(self, in_channels, out_channels, stride):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def bottleneck_block(self, in_channels, out_channels, stride):\n",
    "        bottleneck_channels = out_channels // 4\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, bottleneck_channels, kernel_size=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(bottleneck_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(bottleneck_channels, bottleneck_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(bottleneck_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(bottleneck_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.block(x) + self.shortcut(x))\n",
    "\n",
    "\n",
    "# %%\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes, resnetModel='ResNet50'):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.resnetModel = resnetModel\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        if resnetModel == 'ResNet34':\n",
    "            self.res34layers()\n",
    "        elif resnetModel == 'ResNet50':\n",
    "            self.res50layers()\n",
    "        else:\n",
    "            print('Model not implemented')\n",
    "\n",
    "        \n",
    "    def res34layers(self):\n",
    "        self.conv2_x = self._make_layer(64, 64, n_blocks=3, use_bottleneck=False)\n",
    "        self.conv3_x = self._make_layer(64, 128, n_blocks=4, use_bottleneck=False, downsample=True)\n",
    "        self.conv4_x = self._make_layer(128, 256, n_blocks=6, use_bottleneck=False, downsample=True)\n",
    "        self.conv5_x = self._make_layer(256, 512, n_blocks=3, use_bottleneck=False, downsample=True)\n",
    "\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc = nn.Linear(512, self.num_classes)\n",
    "    def res50layers(self):\n",
    "        self.conv2_x = self._make_layer(64, 256, n_blocks=3, use_bottleneck=True)\n",
    "        self.conv3_x = self._make_layer(256, 512, n_blocks=4, use_bottleneck=True, downsample=True)\n",
    "        self.conv4_x = self._make_layer(512, 1024, n_blocks=6, use_bottleneck=True, downsample=True)\n",
    "        self.conv5_x = self._make_layer(1024, 2048, n_blocks=3, use_bottleneck=True, downsample=True)\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc = nn.Linear(2048, self.num_classes)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, n_blocks, use_bottleneck, downsample=False):\n",
    "        layers = [ResBlocks(in_channels, out_channels, use_bottleneck, downsample)]\n",
    "        for _ in range(1, n_blocks):\n",
    "            layers.append(ResBlocks(out_channels, out_channels, use_bottleneck))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.conv2_x(x)\n",
    "        x = self.conv3_x(x)\n",
    "        x = self.conv4_x(x)\n",
    "        x = self.conv5_x(x)\n",
    "\n",
    "        x = self.global_avg_pooling(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "num_classes = len(train_df['Finding Labels'].unique())\n",
    "model = ResNet(num_classes=num_classes, resnetModel='ResNet34')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=train_df['Finding Labels'].unique(), y=train_df['Finding Labels'])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights,label_smoothing=0.01)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.001, betas=(0.9, 0.999))\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=3, mode='min')\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# %%\n",
    "from tqdm.notebook import tqdm\n",
    "# Training Loop\n",
    "num_epochs = 50\n",
    "avg_train_loss = []\n",
    "avg_train_acc = []\n",
    "avg_val_loss = []\n",
    "avg_val_acc = []\n",
    "\n",
    "with tqdm(total=num_epochs, desc=\"Training Progress\", unit=\"epoch\") as epoch_pbar:\n",
    "    print(\"Init training\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total_train = 0\n",
    "        correct_train = 0\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_train_loss = running_loss / len(train_loader)\n",
    "        avg_train_loss.append(epoch_train_loss)\n",
    "\n",
    "        epoch_train_acc = correct_train / total_train\n",
    "        avg_train_acc.append(epoch_train_acc)\n",
    "\n",
    "        #print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Train acc: {epoch_train_acc:.4f}\")\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                probabilities = torch.softmax(outputs, dim=1)  # Get probabilities\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                all_probs.extend(probabilities.cpu().numpy())\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_loss.append(epoch_val_loss)\n",
    "\n",
    "        epoch_val_acc = correct / total\n",
    "        avg_val_acc.append(epoch_val_acc)\n",
    "\n",
    "        #print(f\"Epoch [{epoch+1}/{num_epochs}], Val Loss: {epoch_val_loss:.4f}, Val acc: {epoch_val_acc:.4f}\")\n",
    "\n",
    "        #if StepLR\n",
    "        #scheduler.step()\n",
    "        #reduce lr on plateau\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Update the epoch progress bar\n",
    "        epoch_pbar.update(1)\n",
    "        \n",
    "\n",
    "print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "class Metrics:\n",
    "    @staticmethod\n",
    "    def calculate_accuracy(y_true, y_pred):\n",
    "        return accuracy_score(y_true, y_pred)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_precision(y_true, y_pred, average=\"weighted\"):\n",
    "        return precision_score(y_true, y_pred, average=average, zero_division=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_recall(y_true, y_pred, average=\"weighted\"):\n",
    "        return recall_score(y_true, y_pred, average=average, zero_division=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_f1_score(y_true, y_pred, average=\"weighted\"):\n",
    "        return f1_score(y_true, y_pred, average=average, zero_division=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_confusion_matrix(y_true, y_pred, labels=None):\n",
    "        return confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_all(y_true, y_pred, average=\"macro\"):\n",
    "        table = PrettyTable([\"Metric\", \"Value\"])\n",
    "        table.add_row([\"Accuracy\", Metrics.calculate_accuracy(y_true, y_pred)])\n",
    "        table.add_row([\"Precision\", Metrics.calculate_precision(y_true, y_pred, average=average)])\n",
    "        table.add_row([\"Recall\", Metrics.calculate_recall(y_true, y_pred, average=average)])\n",
    "        table.add_row([\"F1 Score\", Metrics.calculate_f1_score(y_true, y_pred, average=average)])\n",
    "\n",
    "        return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_metrics(avg_train_loss, avg_train_acc, avg_val_loss, avg_val_acc, num_epochs):\n",
    "    table = PrettyTable()\n",
    "\n",
    "    table.field_names = ['Epoch' , \"Train Loss\", \"Train Accuracy\", \"Validation Loss\", \"Validation Accuracy\"]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        table.add_row([epoch + 1, avg_train_loss[epoch], avg_train_acc[epoch], avg_val_loss[epoch], avg_val_acc[epoch]])\n",
    "\n",
    "    print(\"\\n Training & Validation Metrics:\")\n",
    "    print(table)\n",
    "\n",
    "    return str(table)\n",
    "    \n",
    "table = train_val_metrics(avg_train_loss, avg_train_acc, avg_val_loss, avg_val_acc, num_epochs)\n",
    "\n",
    "print(Metrics.calculate_all(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "\n",
    "all_labels_binarized = label_binarize(all_labels, classes=range(num_classes))\n",
    "auc_scores = []\n",
    "for i in range(num_classes):\n",
    "    auc = roc_auc_score(all_labels_binarized[:, i], np.array(all_probs)[:, i])\n",
    "    auc_scores.append(auc)\n",
    "\n",
    "micro_auc = roc_auc_score(all_labels_binarized, np.array(all_probs), average=\"micro\")\n",
    "macro_auc = roc_auc_score(all_labels_binarized, np.array(all_probs), average=\"macro\")\n",
    "\n",
    "print(f\"Per-class AUC Scores: {auc_scores}\")\n",
    "print(f\"Micro-average AUC: {micro_auc}\")\n",
    "print(f\"Macro-average AUC: {macro_auc}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "def plot_roc(all_labels_binarized, all_probs, auc_scores):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    for i in range(len(auc_scores)):\n",
    "        fpr, tpr, _ = roc_curve(all_labels_binarized[:, i], np.array(all_probs)[:, i])\n",
    "        plt.plot(fpr, tpr, label=f\"Class {i} (AUC = {auc_scores[i]:.2f})\")\n",
    "\n",
    "    # Add diagonal line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    fig = plt.gcf()\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    return fig  \n",
    "roc_fig = plot_roc(all_labels_binarized,all_probs,auc_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_val(num_epochs, avg_train_loss, avg_val_loss, avg_train_acc, avg_val_acc):\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, avg_train_loss, 'b-', label='Train Loss')\n",
    "    plt.plot(epochs, avg_val_loss, 'r-', label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train and Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, avg_train_acc, 'b-', label='Train Accuracy')\n",
    "    plt.plot(epochs, avg_val_acc, 'r-', label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Train and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    fig = plt.gcf()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    return fig\n",
    "train_val_fig = plot_train_val(num_epochs, avg_train_loss, avg_val_loss, avg_train_acc, avg_val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrix(all_labels, all_preds, labels):\n",
    "    labels = [label_to_index[label] for label in label_to_index.keys()]\n",
    "    cm = Metrics.calculate_confusion_matrix(all_labels, all_preds, labels=labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(label_to_index.keys()))\n",
    "    disp.plot(cmap='viridis')\n",
    "    fig = plt.gcf()\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    return fig   \n",
    "cm_fig = plot_confusion_matrix(all_labels, all_preds, labels=filtered_data['Finding Labels'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savefigs(cm_fig, roc_fig, train_val_fig, run_name, results_file):\n",
    "    # Define file paths\n",
    "    cm_plot_file = f\"DNNProject/runs/confusion_matrices/confusion_matrix_{run_name}.png\"\n",
    "    roc_file = f\"DNNProject/runs/roc_curve_{run_name}.png\"\n",
    "    train_val_plot_file = f\"DNNProject/runs/loss_accuracy_plots/loss_accuracy_plot_{run_name}.png\"\n",
    "\n",
    "    # Save Confusion Matrix\n",
    "    try:\n",
    "        cm_fig.savefig(cm_plot_file, bbox_inches='tight')\n",
    "        print(f\"Saved Confusion Matrix: {cm_plot_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save Confusion Matrix: {e}\")\n",
    "    finally:\n",
    "        plt.close(cm_fig)\n",
    "\n",
    "    # Save ROC Curve\n",
    "    try:\n",
    "        roc_fig.savefig(roc_file, bbox_inches='tight')\n",
    "        print(f\"Saved ROC Curve: {roc_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save ROC Curve: {e}\")\n",
    "    finally:\n",
    "        plt.close(roc_fig)\n",
    "\n",
    "    # Save Train-Validation Plot\n",
    "    try:\n",
    "        train_val_fig.savefig(train_val_plot_file, bbox_inches='tight')\n",
    "        print(f\"Saved Train-Val Plot: {train_val_plot_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save Train-Val Plot: {e}\")\n",
    "    finally:\n",
    "        plt.close(train_val_fig)\n",
    "\n",
    "    # Log files in results file\n",
    "    with open(results_file, 'a') as f:\n",
    "        f.write(f\"\\n Confusion Matrix: {cm_plot_file}\")\n",
    "        f.write(f\"\\n ROC Curve: {roc_file}\")\n",
    "        f.write(f\"\\n Train-Val Plot: {train_val_plot_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import datetime\n",
    "run_name = f\"run_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S%f')}\"  # Add microseconds\n",
    "\n",
    "# Log file\n",
    "results_file = 'DNNProject/runs/results_with_ROC.txt'\n",
    "\n",
    "with open(results_file, 'a') as f:\n",
    "    f.write(\"\\n\" + \"=\"*50 + \"\\n\")  # Separator mellom kjøringer\n",
    "    f.write(f\"Run: {run_name}\\n\")\n",
    "    f.write(f\"Model: {model.resnetModel}\\n\")\n",
    "    \n",
    "    f.write(\"Transforms:\\n\")\n",
    "    for transform in train_transforms.transforms:\n",
    "        f.write(f\"  - {transform}\\n\")\n",
    "    if 'scheduler' in globals() or 'scheduler' in locals():\n",
    "        if scheduler is not None:\n",
    "            scheduler_exist = True\n",
    "            f.write(\"Scheduler:\\n\")\n",
    "            f.write(f\"  Type: {type(scheduler).__name__}\\n\")\n",
    "            if hasattr(scheduler, 'get_last_lr'):\n",
    "                f.write(f\"  Initial LR: {scheduler.get_last_lr()}\\n\")\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.StepLR):\n",
    "                f.write(f\"  Step Size: {scheduler.step_size}\\n\")\n",
    "                f.write(f\"  Gamma: {scheduler.gamma}\\n\")\n",
    "            elif isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                f.write(f\"  Mode: {scheduler.mode}\\n\")\n",
    "                f.write(f\"  Patience: {scheduler.patience}\\n\")\n",
    "\n",
    "    else:\n",
    "        scheduler_exist = False\n",
    "        f.write(\"Scheduler: None\\n\")\n",
    "    f.write(f\"Epochs: {num_epochs}\\n\")\n",
    "    f.write(f\"Optimizer:{optimizer.__class__.__name__} \\n \")\n",
    "    f.write(\"Hyperparameters:\\n\")\n",
    "    if optimizer.__class__.__name__ == 'SGD':\n",
    "        for i, param_group in enumerate(optimizer.param_groups):\n",
    "            f.write(f\"  Parameter Group {i + 1}:\\n\")\n",
    "            f.write(f\"    Learning Rate: {param_group['lr']} \\n\")\n",
    "            f.write(f\"    Weight Decay: {param_group['weight_decay']} \\n\")\n",
    "            if 'momentum' in param_group:\n",
    "                f.write(f\"    Momentum: {param_group['momentum']} \\n\")\n",
    "            else:\n",
    "                f.write(f\"    Momentum: Not Available\\n\")\n",
    "    elif optimizer.__class__.__name__ == 'Adam':\n",
    "        for i, param_group in enumerate(optimizer.param_groups):\n",
    "            f.write(f\"  Parameter Group {i + 1}:\\n\")\n",
    "            f.write(f\"    Learning Rate: {param_group['lr']} \\n\")\n",
    "            if 'betas' in param_group:\n",
    "                f.write(f\"    Betas: {param_group['betas']}\\n\")\n",
    "            else:\n",
    "                f.write(f\"    Betas: Not Available\\n\")\n",
    "            f.write(f\"    Weight Decay: {param_group['weight_decay']} \\n\")\n",
    "\n",
    "    f.write(\"\\n Training & Validation Metrics:\\n\")\n",
    "    f.write(table + \"\\n\")\n",
    "    \n",
    "    metrics = str(Metrics.calculate_all(all_labels, all_preds))\n",
    "    f.write(\"\\n Overall Metrics:\\n\")\n",
    "    f.write(metrics + \"\\n\")\n",
    "\n",
    "    f.write(\"\\n AUC-ROC Metrics:\\n\")\n",
    "    f.write(f\"Per-class AUC Scores: {auc_scores}\\n\")\n",
    "    f.write(f\"Micro-average AUC: {micro_auc:.4f}\\n\")\n",
    "    f.write(f\"Macro-average AUC: {macro_auc:.4f}\\n\")\n",
    "    f.write(f\"Plots:\")\n",
    "    savefigs(cm_fig,roc_fig,train_val_fig,run_name,results_file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
